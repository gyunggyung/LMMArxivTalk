# LMMArxivTalk
최신 LLM 관련 논문 스터디
LLM, NLG, Dialogue, Reinforcement learning, Distillation, Efficient, Sentence similarity, multimodal, Stable diffusion, TTS, Gen-1, MLOps ETC...

## 규칙
1. 영어 금지
2. 외국인 금지
3. 1주일에 논문 2개 이상
4. 되는 사람은 10개 이상. 저는 3일 동안 했죠
5. 최대 20분 현장에서 논문 읽기
6. 최대 40분 토론
7. 1시간 스터디 시 바로 나가도 됨
8. 자유롭게
9. 모두연 규칙 붙여넣기
10. 다들 대단한 분들이니 질문 많이
11. 공유 자주
12. 각자 더 뛰어난게 있다는 것을 인지
13. 겸손하기 노력하기 잘하기

## 진행 사항
2023-02-16 11:30 ~ 12:45
- [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602.pdf)
- [GPT Understands, To](https://arxiv.org/pdf/2103.10385.pdf)
- [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/pdf/2109.01247.pdf)

## 후보
앞으로 할만한 논문, 코드, 강의들

### arxiv
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)
- [FLAN: Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652.pdf)
- [T0: Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207.pdf)
- [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688.pdf)
- [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/abs/2302.05206.pdf)
- [Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://paperswithcode.com/paper/exploring-the-benefits-of-training-expert.pdf)
- https://arxiv.org/abs/2212.08073.pdf
- https://arxiv.org/abs/1706.03741.pdf
- https://arxiv.org/pdf/2101.03961.pdf
- https://arxiv.org/abs/2211.05110.pdf
- https://arxiv.org/abs/2109.01247.pdf
- https://arxiv.org/pdf/2301.12597v1.pdf
- https://arxiv.org/pdf/2301.00704v1.pdf
- https://arxiv.org/abs/2302.03011.pdf
- https://arxiv.org/pdf/2101.03961.pdf
- AI의 역사와 발전 관련 논문: https://arxiv.org/abs/2101.07357
- 대규모 언어 모델에 대한 연구 논문: https://arxiv.org/abs/2104.08691

### github
- https://github.com/gyunggyung/DistilKoBiLSTM
- https://github.com/BlinkDL/RWKV-LM
- https://github.com/clovaai/donut

### youtube
- https://youtu.be/IaltsI1BCro
- https://youtu.be/sMPq4cVS4kg
- https://youtu.be/-cnujwUDseU
- https://youtu.be/D71zxGRhuxE
- https://youtu.be/ebjkD1Om4uw :염소:


- https://youtube.com/playlist?list=PLsmJteXozP3oHVB5TCrXEcrfQnInMxkoT

### other
- http://web.stanford.edu/class/cs224u/ 
- https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens
- https://research.runwayml.com/gen1 
- https://docs.google.com/document/u/0/d/1nA3PfQ-BooUpjCYErU-BHYvg2_NazAYJ0mvvmcjG40o/mobilebasic
- https://colab.research.google.com/drive/1kiUvz1TrNJa_MOfOld7DHanv4gZsl7MN
- https://colab.research.google.com/drive/1zGPrh-qxscYU2mvhiv8rrjqEn0WHnOOF?usp=sharing

- OpenAI의 언어 모델 관련 블로그: https://openai.com/blog/tag/language-models/
- 대규모 언어 모델에 대한 기사: https://www.technologyreview.com/2021/08/31/1034253/biggest-language-models-ai-gpt3-openai/

## 참여 인원 소개
1. 염기웅: 저는 여러분을 모으고 프로메우스와 바드의 꿈이라는 책을 쓰는 염기웅입니다. LLM Dialogue Distillation 에 관심과 경험이 있습니다! 경량화 Mlops 서빙 멀티모달 멀티태스크 모델에도 관심이 있습니다. newhiwoong@gmail.com
2. 강수진: 
3. 고현웅: 
4. 박상준: 
5. 김찬란: 
6. 이현재: 
7. 김기현: 
